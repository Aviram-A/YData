{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGxI8C5Vm_-a"
      },
      "source": [
        "# <ins> imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EMPbZFJ2m_-b"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect, LangDetectException\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "# Read data from files  \n",
        "df  = pd.read_parquet('https://github.com/omriallouche/ydata_deep_learning_2021/raw/master/data/metrolyrics.parquet')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j5zo9dbFm_-c"
      },
      "source": [
        "<ins> Keep only metal genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d6P_9SXGm_-c"
      },
      "outputs": [],
      "source": [
        "# df = df[(df['genre'] == 'Metal') | (df['genre'] == 'Rock')]\n",
        "df = df[df['genre'] == 'Pop']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A0f1_OLvm_-c"
      },
      "source": [
        "<ins> Remove non-english songs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zaqh3vEAm_-d"
      },
      "outputs": [],
      "source": [
        "# Assuming your DataFrame is called 'df' and the song column is 'song'\n",
        "# Create a new column 'language' with the detected language for each song name\n",
        "def detect_language(song_name):\n",
        "    try:\n",
        "        return detect(song_name)\n",
        "    except LangDetectException:\n",
        "        return ''\n",
        "\n",
        "df['language'] = df['song'].apply(lambda x: detect_language(x) if pd.notnull(x) else '')\n",
        "\n",
        "# Filter out rows where the language is not English\n",
        "df = df[df['language'] == 'en']\n",
        "\n",
        "# Drop the 'language' column if you don't need it anymore\n",
        "df = df.drop(['language'], axis=1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-G-Eg99zm_-d"
      },
      "source": [
        "<ins> Clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pAzX6EY7m_-d"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "def clean_data(text):\n",
        "    # Remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s\\n]', '', text)\n",
        "\n",
        "    # Remove digits\n",
        "    text = re.sub(r'[0-9]', '', text)\n",
        "\n",
        "    # Remove line breaks and tabs\n",
        "    # text = text.replace('', ' ')\n",
        "    text = text.replace('\\t', ' ')\n",
        "\n",
        "    # Replace multiple subsequent empty spaces with a single space\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    \n",
        "    # Remove non-English characters\n",
        "    text = unidecode(text)\n",
        "\n",
        "    # Lowercase text\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_data(text):\n",
        "    # text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('[^a-zA-Z\\n ]', '', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KioE9Twom_-e"
      },
      "outputs": [],
      "source": [
        "df['sent'] = df['lyrics'].apply(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hDanzA_ym_-e"
      },
      "outputs": [],
      "source": [
        "text = ''\n",
        "\n",
        "for song in df['sent']:\n",
        "    text += song\n",
        "\n",
        "chars = tuple(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "COuGN2-bm_-e"
      },
      "outputs": [],
      "source": [
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8-tEiUzNm_-e"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9aaAp2L-m_-f"
      },
      "outputs": [],
      "source": [
        "def get_batches(arr, n_seqs, n_steps):\n",
        "    '''Create a generator that returns batches of size\n",
        "       n_seqs x n_steps from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       n_seqs: Batch size, the number of sequences per batch\n",
        "       n_steps: Number of sequence steps per batch\n",
        "    '''\n",
        "    \n",
        "    batch_size = n_seqs * n_steps\n",
        "    n_batches = len(arr)//batch_size\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size]\n",
        "    \n",
        "    # Reshape into n_seqs rows\n",
        "    arr = arr.reshape((n_seqs, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], n_steps):\n",
        "        \n",
        "        # The features\n",
        "        x = arr[:, n:n+n_steps]\n",
        "        \n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        \n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <ins> The network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pD8YGL8Jm_-f"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CharRNN(nn.Module):   \n",
        "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # Creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## Define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## Define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## Define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "        \n",
        "        # Initialize the weights\n",
        "        self.init_weights()\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hc):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hc`. '''\n",
        "        \n",
        "        ## Get x, and the new hidden state (h, c) from the lstm\n",
        "        x, (h, c) = self.lstm(x, hc)\n",
        "        \n",
        "        ## pass x through the dropout layer\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Reshape to fit fully-connected layer expected shape\n",
        "        x = x.reshape(-1, self.n_hidden)\n",
        "        \n",
        "        ## Put x through the fully-connected layer\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        # Return x and the hidden state (h, c)\n",
        "        return x, (h, c)\n",
        "    \n",
        "    \n",
        "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "        \n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        if cuda:\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "        \n",
        "        if h is None:\n",
        "            h = self.init_hidden(1)\n",
        "        \n",
        "        x = np.array([[self.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(self.chars))\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = self.forward(inputs, h)\n",
        "\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        \n",
        "        if cuda:\n",
        "            p = p.cpu()\n",
        "        \n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(self.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        p = p.numpy().squeeze()\n",
        "        \n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "            \n",
        "        return self.int2char[char], h\n",
        "    \n",
        "    def init_weights(self):\n",
        "        ''' Initialize weights for fully connected layer '''\n",
        "        initrange = 0.1\n",
        "        \n",
        "        # Set bias tensor to all zeros\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        # FC weights as random uniform\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "        \n",
        "    def init_hidden(self, n_seqs):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <ins> The train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VmJ66XxJm_-f"
      },
      "outputs": [],
      "source": [
        "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
        "        n_steps: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        cuda: Train with CUDA on a GPU\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        \n",
        "        h = net.init_hidden(n_seqs)\n",
        "        \n",
        "        for x, y in get_batches(data, n_seqs, n_steps):\n",
        "            \n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net.forward(inputs, h)\n",
        "            \n",
        "            loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            opt.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                \n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(n_seqs)\n",
        "                val_losses = []\n",
        "                \n",
        "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
        "                    \n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if cuda:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net.forward(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <ins> Run here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnXpz7qZm_-f",
        "outputId": "6e9e4e39-3fea-4ad9-ef43-5a5e8843a7a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 1000... Loss: 1.6238... Val Loss: 1.7020\n",
            "Epoch: 1/10... Step: 2000... Loss: 1.5110... Val Loss: 1.5541\n",
            "Epoch: 2/10... Step: 3000... Loss: 1.4507... Val Loss: 1.4843\n",
            "Epoch: 2/10... Step: 4000... Loss: 1.4239... Val Loss: 1.4382\n",
            "Epoch: 3/10... Step: 5000... Loss: 1.3446... Val Loss: 1.4050\n",
            "Epoch: 3/10... Step: 6000... Loss: 1.3529... Val Loss: 1.3794\n",
            "Epoch: 4/10... Step: 7000... Loss: 1.3346... Val Loss: 1.3738\n",
            "Epoch: 4/10... Step: 8000... Loss: 1.3367... Val Loss: 1.3536\n",
            "Epoch: 5/10... Step: 9000... Loss: 1.2645... Val Loss: 1.3475\n",
            "Epoch: 5/10... Step: 10000... Loss: 1.2480... Val Loss: 1.3403\n",
            "Epoch: 6/10... Step: 11000... Loss: 1.2444... Val Loss: 1.3388\n",
            "Epoch: 6/10... Step: 12000... Loss: 1.1649... Val Loss: 1.3234\n",
            "Epoch: 7/10... Step: 13000... Loss: 1.2388... Val Loss: 1.3235\n",
            "Epoch: 7/10... Step: 14000... Loss: 1.1117... Val Loss: 1.3192\n",
            "Epoch: 8/10... Step: 15000... Loss: 1.2011... Val Loss: 1.3179\n",
            "Epoch: 8/10... Step: 16000... Loss: 1.1822... Val Loss: 1.3123\n",
            "Epoch: 9/10... Step: 17000... Loss: 1.1347... Val Loss: 1.3135\n",
            "Epoch: 9/10... Step: 18000... Loss: 1.1529... Val Loss: 1.3127\n",
            "Epoch: 10/10... Step: 19000... Loss: 1.2051... Val Loss: 1.3019\n",
            "Epoch: 10/10... Step: 20000... Loss: 1.2471... Val Loss: 1.3043\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize and print the network\n",
        "net = CharRNN(chars, n_hidden=512, n_layers=1,drop_prob=0.5)\n",
        "n_seqs, n_steps = 128, 20\n",
        "\n",
        "train(net, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IRHtVpqQm_-f"
      },
      "outputs": [],
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_1_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 50... Loss: 1.2032... Val Loss: 1.2619\n",
            "Epoch: 1/10... Step: 100... Loss: 1.2037... Val Loss: 1.2530\n",
            "Epoch: 1/10... Step: 150... Loss: 1.2058... Val Loss: 1.2497\n",
            "Epoch: 1/10... Step: 200... Loss: 1.1686... Val Loss: 1.2505\n",
            "Epoch: 2/10... Step: 250... Loss: 1.1363... Val Loss: 1.2495\n",
            "Epoch: 2/10... Step: 300... Loss: 1.1337... Val Loss: 1.2446\n",
            "Epoch: 2/10... Step: 350... Loss: 1.1417... Val Loss: 1.2453\n",
            "Epoch: 2/10... Step: 400... Loss: 1.1101... Val Loss: 1.2484\n",
            "Epoch: 3/10... Step: 450... Loss: 1.1085... Val Loss: 1.2477\n",
            "Epoch: 3/10... Step: 500... Loss: 1.1326... Val Loss: 1.2479\n",
            "Epoch: 3/10... Step: 550... Loss: 1.0844... Val Loss: 1.2466\n",
            "Epoch: 3/10... Step: 600... Loss: 1.0589... Val Loss: 1.2492\n",
            "Epoch: 4/10... Step: 650... Loss: 1.1199... Val Loss: 1.2500\n",
            "Epoch: 4/10... Step: 700... Loss: 1.1211... Val Loss: 1.2468\n",
            "Epoch: 4/10... Step: 750... Loss: 1.0737... Val Loss: 1.2459\n",
            "Epoch: 4/10... Step: 800... Loss: 1.0912... Val Loss: 1.2476\n",
            "Epoch: 5/10... Step: 850... Loss: 1.1202... Val Loss: 1.2477\n",
            "Epoch: 5/10... Step: 900... Loss: 1.1111... Val Loss: 1.2462\n",
            "Epoch: 5/10... Step: 950... Loss: 1.1360... Val Loss: 1.2476\n",
            "Epoch: 5/10... Step: 1000... Loss: 1.0963... Val Loss: 1.2464\n",
            "Epoch: 6/10... Step: 1050... Loss: 1.0967... Val Loss: 1.2524\n",
            "Epoch: 6/10... Step: 1100... Loss: 1.0864... Val Loss: 1.2504\n",
            "Epoch: 6/10... Step: 1150... Loss: 1.0996... Val Loss: 1.2473\n",
            "Epoch: 6/10... Step: 1200... Loss: 1.1436... Val Loss: 1.2488\n",
            "Epoch: 7/10... Step: 1250... Loss: 1.1087... Val Loss: 1.2513\n",
            "Epoch: 7/10... Step: 1300... Loss: 1.1120... Val Loss: 1.2483\n",
            "Epoch: 7/10... Step: 1350... Loss: 1.0923... Val Loss: 1.2472\n",
            "Epoch: 7/10... Step: 1400... Loss: 1.1222... Val Loss: 1.2466\n",
            "Epoch: 8/10... Step: 1450... Loss: 1.1304... Val Loss: 1.2516\n",
            "Epoch: 8/10... Step: 1500... Loss: 1.1019... Val Loss: 1.2496\n",
            "Epoch: 8/10... Step: 1550... Loss: 1.1059... Val Loss: 1.2494\n",
            "Epoch: 8/10... Step: 1600... Loss: 1.1458... Val Loss: 1.2615\n",
            "Epoch: 8/10... Step: 1650... Loss: 1.1153... Val Loss: 1.2547\n",
            "Epoch: 9/10... Step: 1700... Loss: 1.0889... Val Loss: 1.2496\n",
            "Epoch: 9/10... Step: 1750... Loss: 1.0802... Val Loss: 1.2483\n",
            "Epoch: 9/10... Step: 1800... Loss: 1.1063... Val Loss: 1.2475\n",
            "Epoch: 9/10... Step: 1850... Loss: 1.1052... Val Loss: 1.2503\n",
            "Epoch: 10/10... Step: 1900... Loss: 1.1154... Val Loss: 1.2496\n",
            "Epoch: 10/10... Step: 1950... Loss: 1.1000... Val Loss: 1.2518\n",
            "Epoch: 10/10... Step: 2000... Loss: 1.0622... Val Loss: 1.2530\n",
            "Epoch: 10/10... Step: 2050... Loss: 1.0199... Val Loss: 1.2567\n"
          ]
        }
      ],
      "source": [
        "# Initialize a new CharRNN instance\n",
        "net = CharRNN(chars, n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load('rnn_1_epoch.net')\n",
        "\n",
        "# Load the state dictionary into the new instance\n",
        "net.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "n_seqs, n_steps = 128, 200\n",
        "\n",
        "train(net, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aKAUPV0_m_-f"
      },
      "outputs": [],
      "source": [
        "def sample(net, size, prime='balls', top_k=None, cuda=False):\n",
        "        \n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    \n",
        "    h = net.init_hidden(1)\n",
        "    \n",
        "    for ch in prime:\n",
        "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        \n",
        "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BIvMTHyNm_-f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rap is about life and life is about hardship\n",
            "if youre the only one\n",
            "this is the distance\n",
            "with me\n",
            "and im gonna be this is all about you baby this time\n",
            "and i see its together inside\n",
            "the money in the same of you i still got the way were so lonely\n",
            "im looking for the track in the dark\n",
            "we stay and so long we got one to me\n",
            "talking to the moon out on the wass on somebody\n",
            "all ive been so fire\n",
            "i want the truth we could de you told you the same\n",
            "im true light is that fall\n",
            "and it all been someone to me\n",
            "if you shell feel a shell\n",
            "so hand to hand on and bri\n"
          ]
        }
      ],
      "source": [
        "print(sample(net, 500, prime='rap is about life and life is about hardship\\n', top_k=5, cuda=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
